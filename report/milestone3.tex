\documentclass[11pt]{article}
\usepackage[a4paper, portrait, margin=1in]{geometry}
\usepackage{mathtools}
\usepackage{listings}
\usepackage[dvipsnames]{xcolor}
\usepackage{color}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[colorlinks=true,urlcolor=blue,linkcolor=black]{hyperref}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}

\fancypagestyle{firstpagefooter}
{
\lfoot{Compiled: \today}
\cfoot{}
\rfoot{\thepage}
}
\cfoot{\thepage}


\renewcommand{\headrulewidth}{0pt}
\renewcommand{\footrulewidth}{0pt}


\newcommand{\code}[1]{\lstinline[language=Java]{#1}}
\newcommand{\get}[0]{\texttt{GET}}
\newcommand{\set}[0]{\texttt{SET}}
\newcommand{\todo}[1]{\fcolorbox{black}{Apricot}{TODO: #1}}
\newcommand{\linkmain}[1]{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/blob/master/src/main/java/asl/#1.java}{#1}}
\newcommand{\linktest}[1]{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/blob/master/src/test/java/asl/#1.java}{#1}}

\newcommand{\resultsurl}[1]{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/blob/master/results/#1}{gitlab.inf.ethz.ch/.../results/#1}}


\begin{document}

\title{Advanced Systems Lab (Fall'16) -- Third Milestone}

\author{Name: \emph{Taivo Pungas}\\Legi number: \emph{15-928-336}}

\date{
\vspace{4cm}
\textbf{Grading} \\
\begin{tabular}{|c|c|}
\hline  \textbf{Section} & \textbf{Points} \\ 
\hline  1 &  \\ 
\hline  2 &  \\ 
\hline  3 &  \\ 
\hline  4 &  \\ 
\hline  5 &  \\ 
\hline \hline Total & \\
\hline 
\end{tabular} 
}

\maketitle
\thispagestyle{firstpagefooter}
\newpage

\tableofcontents

\clearpage
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\section{System as One Unit}\label{sec:part1-system-one-unit}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------

\subsection{Data}

The experimental data used in this section comes from the updated trace experiment, found in \texttt{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/tree/master/results/trace\_rep3}{results/trace\_rep3}} (short names \texttt{trace\_ms*}, \texttt{trace\_mw} and \texttt{trace\_req} in Milestone~1). For details, see Milestone~2, Appendix A.

The first 2 minutes and last 2 minutes were dropped as warm-up and cool-down time similarly to previous milestones.

\subsection{Model}
\label{sec:part1:model}

In this section I create an M/M/1 model of the system. This means the following definitions and assumptions:
\begin{itemize}
	\item The queues are defined as having infinite buffer capacity.
	\item The population size is infinite.
	\item The service discipline is FCFS.
	\item Interarrival times and the service times are exponentially distributed.
	\item We treat the SUT as a single server and as a black box.
	\item Arrivals are individual, so we have a birth-death process.
\end{itemize}
 
\paragraph{Parameter estimation}
Using the available experimental data, it is not possible to directly calculate the mean arrival rate $\lambda$ and mean service rate $\mu$ so we need to estimate them somehow. I estimated both using throughput of the system: I take $\lambda$ to be the \emph{mean} throughput over 1-second windows, and $\mu$ to be the the \emph{maximum} throughput in any 1-second window, calculated from middleware logs. I chose a 1-second window because a too small window is highly susceptible to noise whereas a too large window size drowns out useful information.

\todo{mention that throughput statistics are not affected by the network (even though when calculating service times etc, I would want to exclude network times)}

\paragraph{Problems}
The assumptions above obviously do not hold for our actual system. Especially strong is the assumption of a single server; since we actually have multiple servers, this model is likely to predict the behaviour of the system very poorly. A second problem arises from my very indirect method of estimating parameters for the model (and an arbitrary choice of time window) which introduces inaccuracies.

\subsection{Comparison of model and experiments}

Explain the characteristics and behavior of the model built, and compare it with the experimental data (collected both outside and inside the middleware). Map the similarities and differences to aspects of the design or the experiments.

Table~\ref{tbl:part1:comparison_table} shows a comparison of the predictions of the M/M/1 model with actual results from the trace experiment. 

\todo{this whole subsection}

\todo{mention that IRTL doesn't hold for the model}

\input{../results/analysis/part1_mm1/comparison_table.txt}

\begin{figure}[h]
\includegraphics[width=\textwidth]{../results/analysis/part1_mm1/graphs/response_time_quantiles_actual_and_predicted.pdf}
\caption{Quantiles of the response time distribution: experimental results and predictions of the M/M/1 model. Note the extreme difference in the response time scale.}
\label{fig:part1:quantiles_responsetime}
\end{figure}

\todo{mention} that number of jobs is calculated using Little's law

\begin{figure}[h]
\includegraphics[width=\textwidth]{../results/analysis/part1_mm1/graphs/queue_time_quantiles_actual_and_predicted.pdf}
\caption{Quantiles of the waiting time distribution: experimental results and predictions of the M/M/1 model. Note the extreme difference in the queue time scale.}
\label{fig:part1:quantiles_queuetime}
\end{figure}


\clearpage
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\section{Analysis of System Based on Scalability Data}\label{sec:part2-analysis-scalability}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------

\subsection{Data}

The experimental data used in this section comes from Milestone~2 Section~1 and can be found in \texttt{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/tree/master/results/throughput}{results/throughput}}.


\subsection{Model}

The assumptions and definitions of the M/M/m model are the same as for the M/M/1 model laid out in Section~\ref{sec:part1:model} with the following modifications:

\begin{itemize}
	\item We treat the SUT as a collection of $m$ servers.
	\item All jobs waiting for service are held in one queue.
	\item If any server is idle, an arriving job is serviced immediately.
	\item If all servers are busy, an arriving job is added to the queue.
\end{itemize}

\paragraph{Parameters}
\todo{} describe how I found the parameters


\paragraph{Problems}
\todo{}

\begin{enumerate}
	\item I actually have $m$ queues (one for each server), not a single queue; each request is assigned to a server when \linkmain{LoadBalancer} receives it.
	\item I map requests to servers uniformly. M/M/m assumes that each server takes a request when it finishes with the previous one, but that is not true in my case -- I take earlier
\end{enumerate}

\subsection{Comparison of model and experiments}

\todo{}

\begin{figure}[h]
\centering
\includegraphics[width=0.5\textwidth]{../results/analysis/part2_mmm/graphs/traffic_intensity_vs_clients.pdf}
\caption{\todo{}}
\label{fig:part2:trafficintensity}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../results/analysis/part2_mmm/graphs/response_time_predicted_and_actual.pdf}
\caption{\todo{} note difference in scale}
\label{fig:part2:responsetime}
\end{figure}

\input{../results/analysis/part2_mmm/comparison_table.txt}


\clearpage
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\section{System as Network of Queues}\label{sec:part3-network-of-queues}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------

\subsection{Guidelines}
Length: 1-3 pages

Based on the outcome of the different modeling efforts from the previous sections, build a comprehensive network of queues model for the whole system. Compare it with experimental data and use the methods discussed in the lecture and the book to provide an in-depth analysis of the behavior. This includes the identification and analysis of bottlenecks in your system. Make sure to follow the model-related guidelines described in the Notes!


\clearpage
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\section{Factorial Experiment}\label{sec:part4-2k-experiment}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------

\subsection{Experimental question and experiment design}
The goal of this section is to find out the factors that influence throughput of the system. In particular, I will investigate the effect of $k=3$ factors -- $S$ (number of servers), $R$ (replication level) and $W$ (percentage of writes in workload) -- on total throughput of the system. Everything else will be kept fixed: the number of threads $T=32$ and the number of clients $C=180$.

For each factor in $\{S,R,W\}$ we need to pick two levels. Since I expect throughput to monotonically increase with $S$, decrease with $R$, and decrease with $W$, we can use the minimum and maximum level for each factor in our $2^k$ experiment: $S \in \{3, 7\}$, $R \in \{1, S\}$, and $W \in \{1, 10\}$.


\subsection{Data}

The experimental data used in this section comes from Milestone~2 Section~3 and can be found in \texttt{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/tree/master/results/writes}{results/writes}} (short name \texttt{writes-S*-R*-W*-r*} in Milestone~2). For each combination of $S$, $R$, and $W$ we have $r=3$ repetitions. This means data from $2^k \cdot r = 24$ distinct runs are used in this section.

The data table is available in \hyperref[sec:appa]{Appendix~A}.

%\input{../results/analysis/part4_2k/data_table.txt}

\subsection{Results}

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{../results/analysis/part4_2k/graphs/error_vs_predicted_tps.pdf}
\caption{Error of the $2^k$ model as a function of predicted throughput. Each point is one repetition of a configuration. Note the throughput axis does not include zero.}
\label{fig:part4:errorvspredicted}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\centering
\includegraphics[width=\textwidth]{../results/analysis/part4_2k/graphs/dist_vs_level.pdf}
\caption{Boxplot of the error distribution for all levels of all factors. The box shows the inter-quartile range (between 25th and 75th percentiles, IQR), with the median shown as a horizontal line. The top and bottom whisker show the highest and lowest value within $1.5 IQR$ of the median. Experiment results are also plotted as single points. Note that each subplot contains all 24 data points.}
\label{fig:part4:errordistvslevel}
\end{subfigure}
\caption{Evaluation of the $2^k$ model.}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../results/analysis/part4_2k/graphs/error_vs_order.pdf}
\caption{Error of the $2^k$ model as a function of the order in which experiments were run. Color of the points shows the repetition number.}
\label{fig:part4:errorvsorder}
\end{figure}


\subsubsection{Checking assumptions}

Before delving into the predictions of model we need to check whether assumptions we made in modelling actually hold.

\paragraph{Independent errors}

The model assumes that errors are independently and identically distributed (IID). Figure~\ref{fig:part4:errorvspredicted} shows that error does not depend on the predicted throughput. Furthermore, errors do not depend on factor levels as shown in Figure~\ref{fig:part4:errordistvslevel}: the median and 25\% and 75\% percentiles of the error distribution are independent of the factor and the level.

Figure~\ref{fig:part4:errorvsorder} shows that errors clearly depend on repetition. An obvious hypothesis here is that each successive repetition improved the throughput for some reason. However, repetition 2 was run on Nov 20, and both repetitions 3 and 4 were run on Nov 23 with a 6-hour difference; the resource group was hibernated and redeployed before each experiment. For this reason we can't accept the hypothesis that repetitions somehow affected each other.

Thus, the large variation in throughput between experiments can be explained in two ways: it could a) be caused by differing conditions in Azure between deployments, or b) be somehow inherent to the SUT. If b) were true, we would also see large variance \emph{within} each deployment, which is not the case. This leaves us with option a): the conditions on Azure differed between employments -- possibly due to different server or network allocation, or the total load in Azure, or some other factor.

\paragraph{Normally distributed errors}

The model assumes that errors are normally distributed. The quantile-quantile plot in Figure~\ref{fig:part4:quantile_quantile} shows that this is clearly not the case -- the lowest quantiles are too low and medium-to-high quantiles too high for the distribution to be normal. This, however, is caused by the trimodality of the error distribution: the throughputs (and thus, errors) of each repetition individually are distributed much more closely to a normal distributions, but when we concatenate the repetitions, the result is trimodal.

\begin{figure}
\centering
\includegraphics[width=0.5\textwidth]{../results/analysis/part4_2k/graphs/quantile_quantile.pdf}
\caption{Quantiles of the residual distribution plotted against quantiles of the standard normal distribution. The straight line shows the best fit of a linear trendline through the points.}
\label{fig:part4:quantile_quantile}
\end{figure}


\paragraph{Constant standard deviation} As Figure~\ref{fig:part4:errordistvslevel} shows, the distribution looks similar at all factor levels, so the model assumption of a constant standard deviation holds.


\subsubsection{Checking model fit}


\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{../results/analysis/part4_2k/graphs/actual_and_predicted_vs_servers_and_writes.pdf}
\caption{Throughput as a function of $W$. Each line with points shows the results of one repetition. The red line shows the throughput predicted by the $2^k$ model; the light red ribbon shows the standard deviation that we would expect to see according to the model, if we were to run 3 repetitions.}
\label{fig:part4:actual_and_predicted_vs_servers_and_writes}
\end{figure}


\todo{} analyse how good our model fit is -- how big is error etc

Book: "The residuals are an order of magnitude smaller than the responses, and there does not appear to be any definite trend in the mean or spread of the residuals."

multiplicative model is not necessary because: a) range of values covered is small, residuals are small compared to actual values and the spread of residuals is independent of whether actual values are small or large \todo{check if this is true}

additive model => mean error 1665; multiplicative => mean error 1667

\input{../results/analysis/part4_2k/coefficient_and_var_table.txt}

\subsubsection{Analysis of the model}
\todo{} 

noise is obv biggest, writes is most significant, x\_ab also a tiny bit. others are all an order of magnitude smaller than writes.

\clearpage
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\section{Interactive Law Verification}\label{sec:part5-interactive-law}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------

\subsection{Data}

The experimental data used in this section comes from Milestone~2, Section~2 (Effect of Replication) and can be found in \texttt{\href{https://gitlab.inf.ethz.ch/pungast/asl-fall16-project/tree/master/results/replication}{results/replication}} (short name \texttt{replication-S*-R*-r*} in Milestone~2). This includes a total of 27 experiments in 9 different configurations.

The first 2 minutes and last 2 minutes were \textbf{not} dropped because the Interactive Response Time Law (IRTL) should hold also in warm-up and cool-down periods. Repetitions at the same configuration were considered as separate experiments.

\subsection{Model}

We are assuming a closed system, i.e. clients wait for a response from the server before sending another request. Under this assumption, the IRTL should hold:

$$R = \frac{N}{X} - Z$$

where $R$ is mean response time, $Z$ is waiting time in the client, $N$ is the number of clients and $X$ is throughput.

\subsection{Results}

Using IRTL, we can verify the validity of experiments by calculating the predicted throughput $X_{predicted}$ (given the number of clients $C$ and mean response time $R$) and comparing it with actual throughput $X_{actual}$. This is precisely what I did for all experiments of Milestone~2, Section~2. $C=180$ in all experiments, and both $R$ and $X_{actual}$ are aggregated results reported by the three memaslap instances generating load in that experiment.

\begin{figure}[h]
\centering
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[width=\textwidth]{../results/analysis/part5_irtl/graphs/error_percentage.pdf}
\caption{Histogram of the relative error of throughput predicted using IRTL, counting the number of experiments in a given error range. Note the horizontal scale does not include 0.}
\label{fig:part5:error_percentage}
\end{subfigure}
\begin{subfigure}[t]{0.49\textwidth}
\includegraphics[width=\textwidth]{../results/analysis/part5_irtl/graphs/predicted_vs_actual_throughput.pdf}
\caption{Throughput predicted using IRTL (dark points), as a function of actual throughput calculated from experimental data. The red line shows hypothetical perfect predictions (the $x=y$ line). Note the horizontal scale does not include 0.}
\label{fig:part5:predicted_vs_actual}
\end{subfigure}
\caption{Evaluation of the validity of Milestone~2 Section~2 experiments}
\end{figure}

If we assume the wait time $Z$ to be 0, we get a mean relative prediction error of $-1.01\%$, defined as $\frac{X_{predicted}-X_{actual}}{X_{actual}}$. The distribution of these errors is shown in Figure~\ref{fig:part5:error_percentage}; the distribution looks reasonably symmetric. Figure~\ref{fig:part5:predicted_vs_actual} plots $X_{predicted}$ against $X_{actual}$ and shows again that the predicted throughput is very close to actual throughput, but consistently smaller in all regions of the graph.

If we assume a nonzero $Z$ and estimate it from the experiments, we get a mean estimated wait time of -0.111ms. Clearly this is impossible: wait time must be non-negative.

To explain these results, we need to answer the question: why is the actual throughput lower than the actual throughput? It could be that memaslap starts the clock for a new request before stopping the clock for the previous request -- which would violate the closed system assumption -- but this is very unlikely as it would be a major design flaw in memaslap.

A more plausible hypothesis is that the effective value of $N$ is slightly lower than the concurrency I set using the relevant command line flags -- because the number of cores in the memaslap machine is much lower than the concurrency I'm using.

Regardless of the exact reason of the deviation, the IRTL holds to a reasonably high accuracy. Perfect accuracy is impossible even without the middleware: in the baseline experiments (Milestone~1 Section~2), relative prediction error is 0.2\%-0.5\% in a selection of values of $N$ that I checked.

\todo{if time} think about what could be causing the -1\%, but not a priority


% potential reasons from slides: network effects, processing overhead, exceptions and non-normal return values



% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
% --------------------------------- Appendices -----------------------------------
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------

\clearpage

\section*{Appendix A: Data for the factorial experiment}
\label{sec:appa}
\addcontentsline{toc}{section}{Appendix A: Data for the factorial experiment}

\input{../results/analysis/part4_2k/data_table.txt}

\end{document}